{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "source": [
        "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq0S-Fjg2KEs",
        "colab_type": "text"
      },
      "source": [
        "#32513 Advanced Data Analytics Algorithms\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Spring 2019\n",
        "\n",
        "###Assignment 2\n",
        "###Practical Workplace-Related Data Analytics Project\n",
        "\n",
        "###Name:Prasad Desai - 13057765\n",
        "###Priyanka Raikar - 12848800\n",
        "     \n",
        "     \n",
        "###Date: 25/09/2019\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. Introduction \n",
        "\n",
        "## 1.1 Business Description \n",
        "\n",
        " \n",
        "\n",
        "Machine learning can be utilized in various real-life applications, a model was created to anticipate the punching shear quality of slabs without shear support. The model was created utilizing a database gathered from construction sites. Four key parameters were utilized to assemble the model, which were slab width, concrete weight, slab height which gives strength of the slab in three categories. The yield parameter of the model was strength of the slab. The outcomes from various model were contrasted with those from the disentangled strength conditions and furthermore through exploratory outcomes. The root means square mistake (RMSE) and the connection coefficient (R) were utilized as assessment criteria. Parametric examinations were displayed utilizing machine learning models to survey the impact of each information parameter on the punching shear quality and to contrast results with those from the conditions proposed in regularly utilized codes. The outcomes demonstrated that one of the models is straightforward and given the most exact forecasts of the quality of slab. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 1.2. Defining Problem \n",
        "\n",
        " \n",
        "\n",
        "In civil engineering and industrial facilities, concrete mixture is a vital structural material, the strength of the concreate is the predominant factor considered in the process. The strength of the concrete gets deteriorate because of external factors like extreme environmental conditions and exposure. In harsh environments, the exposed concrete is likely to loose its strength causing accidental incidents. Hence, the punching sheer quality of slab is the most  important in safety assessment for concrete slabs. The testing and monitoring of the quality indicator i.e. concrete slab strength during the manufacturing process will help in taking advantageous measures and performance testing.  \n",
        "\n",
        "Therefore, accurate prediction of concrete strength will be beneficiary in increasing safety of the structures and assessing the strength of the slab. It is very much essential explore methods which will predict and estimate concrete strength.  The traditional mathematic forecasting methods needs huge amount of data, hence there are lot of influencing parameters rising the complexity of the degradation mechanism. The most effective way to predict compressive punching sheer quality of concrete slab strength can be performed through simplified machine learning models for predictive analysis.  \n",
        "\n",
        "## 1.3 AIM \n",
        "\n",
        "The study aims to conduct analysis to predict the punching shear strength of concrete slab by classifying slab strength into three different categories like low, medium and high. The classification is based on numerical calculations performed on data attributes by different machine learning algorithms which predicts the accuracy, the result is clustered and represented. \n",
        "\n",
        "# 2.Algorithm Implementation\n",
        "## 2.1 Inputs & Outputs \n",
        "\n",
        "The dataset used for machine learning algorithm contains features of concrete slab like length, width, height and weight.  \n",
        "\n",
        "| Sr. no. | Attribute     | Data type|Measure    | \n",
        "|---------|---------------|----------|-----------|\n",
        "|   1     |Slab_height    |integer   |inches     |\n",
        "|   2     |Slab_width     |integer   |centimetres|\n",
        "|   3     |Slab_weight    |integer   |pounds     |\n",
        "|   4     |Slab_length    |integer   |centimetres| \n",
        "\n",
        "Table 1: Data Dictionary\n",
        "\n",
        "\n",
        "\\\n",
        "The simplicity of data helped in resolving clustering and classification problem. There was no need to perform data cleaning operations.   \n",
        "\n",
        "The output generated by machine learning algorithm is the strength of the slab, Slab_Strength is clustered three different groups i.e. low medium high.   \n",
        "\n",
        "## 2.2 Algorithm Implementation \n",
        "\\\n",
        "\n",
        "The machine learning algorithms has been implemented in Jupyter IDE using Python. The program was initiated by importing libraries like scipy, numpy, matplot, pandas, sk-Learn as shown in the figure below. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig1.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 1: Collection of libraries imported. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The figure 2 shows python version with latest libraries used for machine learning algorithm implementation. \n",
        "\n",
        "  <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig2.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 2: Python version. \n",
        "\n",
        "In the next stage data from excel sheet has been imported as seen in figure below.  \n",
        "\n",
        "  <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig3.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 3: Code Setup \n",
        "\n",
        "# 3. Exploration \n",
        "\n",
        "## 3.1 Exploratory Data Analysis  \n",
        "\n",
        "The primary investigation was performed on data with an intend to discover patterns, recognise anomalies and hypothesis. The two types of analysis performed were Univariate and Multivariate. \n",
        "\n",
        "Univariate analysis is conducted using one variable, histogram and box plot 1 is graphical representation shown below; On the other hand box plot 2, heat map, pair plots are examples of multivariate EDA. The result of this critical process was represented using graphical visualisation techniques explained below: \n",
        "\n",
        "#### Histogram \n",
        "\n",
        "The histogram was created for four different variables, the graphical display of displays bars of different height. The bars represent estimation of values concentrated in fixed range, for example slab height has maximum data values more than 30. In slab width, a gap is observed between values 2 and 4. The fluctuation in the height of the bars depicts the variance observed in the data values. \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig4.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 4: Histogram  \n",
        "\n",
        "#### Box plot 1 \n",
        "\n",
        "Box plot helps in graphical representation of group of numerical data, in descriptive analysis it is important to visualise data to identify cluster and outliers. Box plot 1 is univariate as four different box plots are obtained for different variables. The upper and lower quartile value differ with variables. \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig5.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 5: Box plot 1 \n",
        "\n",
        "#### Box plot 2 \n",
        "\n",
        "The box plot shown below is bivariate as we can see two different variables namely slab length on y axis and slab strength which is predicted output on x axis.  \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig6.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 6: Box plot 2 \n",
        "\n",
        " \n",
        "\n",
        "#### Heat Map\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "Heat map is an immediate summary of information provided through graphical representation. The data variables are represented in a matrix, darker the colour more is the value. \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig7.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 7: Heat map \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "#### Multivariate plots \n",
        "\n",
        "Multivariate plots shows the interaction between variables, the scatter plots are generated for all possible pair of attributes. It helps in identifying the structured relation between different input variables. \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig8.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 8: Multivariate plots \n",
        "\n",
        "#### Pair plot \n",
        "\n",
        "Pair plots are representation of high-level scatter plot, it records and displays relation between multiple variables from the same dataset. We can observe in the figure below, the attribute slab length is separated from other attributes in all combination of features.  \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig9.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 9: Pair plot \n",
        "\n",
        "#### Joint plot \n",
        "\n",
        "It is a type of marginal plot which allows us to study relation between two different variables. The data used to represent joint plot is numeric, we have used slab weight and slab height. \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig10.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 10: Joint plot \n",
        "\n",
        "## 3.2 Planning and Testing of data models \n",
        "\n",
        "The data models were created and tested in following steps: \n",
        "\n",
        "#### Creating validation dataset \n",
        "\n",
        "In supervised learning methods, it is important to divide the dataset into two parts – testing and training dataset. As shown in figure below, the dataset has been split into 80% and 20%, initially training dataset is trained to prepare models, further it is tested on validation dataset. \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig11.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 11 \n",
        "\n",
        " \n",
        "\n",
        "#### Test harness \n",
        "\n",
        "A 10 fold cross validation method was used, where the dataset was divided into 10 parts so that 9 orders were trained and one part was tested. The process was repeated for all combination of train and test dataset. \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig12.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 12 \n",
        "\n",
        "#### Building Models \n",
        "\n",
        "The approach followed was to experiment different machine learning algorithm models on the dataset. We chose linear and non-linear algorithms to get the best results.  \n",
        "\n",
        "The evaluation was performed on following algorithms: \n",
        "\n",
        "Logistic Regression (LR) \n",
        "\n",
        "Linear Discriminant Analysis (LDA) \n",
        "\n",
        "K-Nearest Neighbours (KNN) \n",
        "\n",
        "Classification and Regression Trees (CART) \n",
        "\n",
        "Guassian Naïve Bayes (NB) \n",
        "\n",
        "Support Vector Machines (SVM) \n",
        "\n",
        "The random number seed was reset before running different models for better evaluation. \n",
        "\n",
        "#### Selection of best model \n",
        "\n",
        "Out of six different model, accuracy estimation was performed based on prediction factors like accuracy, f-score, support, recall. \n",
        "\n",
        " \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig13.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 13 \n",
        "\n",
        "# 4 .Methodology  \n",
        "\n",
        "## 4.1Build and Train Data Models \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig14.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 14  \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig15.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 15 \n",
        "\n",
        " \n",
        "\n",
        "Let’s evaluate 6 different algorithms: \n",
        "\n",
        "Logistic Regression (LR)\n",
        "\n",
        "Logistic regression utilizes a condition as the portrayal, especially like a direct relapse. \n",
        "\n",
        "input labels (x) are consolidated straightly utilizing loads or coefficient labels to foresee an outcome value (y). A key contrast from linear regression is that the yield worth being demonstrated is a parallel quality (0 or 1) instead of a numeric worth.\n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig16.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 16 \n",
        "\n",
        " \n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig17.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 17 \n",
        "\n",
        "Gaussian Naive Bayes (NB)\n",
        "\n",
        "Naive Bayes is an arrangement calculation for two-fold (two-class) and multi-class grouping issues. The procedure is simplest to comprehend when depicted utilizing parallel or linear inputs. \n",
        "\n",
        "Bayes in light of the fact that the count of the probabilities for every theory are disentangled to make their estimation tractable. Instead of endeavoring to figure the estimations of each property estimation P(d1, d2, d3|h), they are thought to be restrictively autonomous given the objective worth and determined as P(d1|h) * P(d2|H, etc.\n",
        " \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig18.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 18 \n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig19.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 19 \n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "In SVM, it is anything but difficult to have a direct hyper-plane between two classes. However, another consuming inquiry which emerges is, should we have to add this component physically to have a hyper-plane. SVM has a strategy called the kernel which is tricky. These are capacities which takes low dimensional information space and change it to a higher dimensional space for example it changes over not detachable issue to distinct issue, these capacities are called pieces. It is for the most part helpful in non-straight partition issue. Basically, it does some incredible information changes, at that point discover the procedure to isolate the information dependent on the marks or yields you've characterized.\n",
        "\n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig20.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 20 \n",
        "\n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig21.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 21\n",
        "\n",
        "Linear Discriminant Analysis (LDA) \n",
        "\n",
        "Classification and Regression Trees (CART).  \n",
        "\n",
        " \n",
        "\n",
        "This is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms. We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable. \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "# 5.Evaluation \n",
        "\n",
        "## 5.1 Report Execution on Data \n",
        "\n",
        "#### K- Nearest Neighbour is the best model \n",
        "\n",
        "K-nearest neighbor is among the easiest and basic form of the supervised machine learning algorithm. It classifies the cases based on their proximity in terms of distance with other similar cases. In other words, similar cases are closer to each other while the non-similar cases are far from each other. Hence, distance is a measure for their similarity or dissimilarity and based on that specifies the number of nearest neighbors. There are two different distance metric to measure similar cases.  \n",
        "\n",
        "• Euclidean is the ordinary straight-line distance between two data points. \n",
        "\n",
        " • Manhattan is the distance between two points is the sum of the absolute difference between two cartesian coordinates. The algorithms assign a class to the new data based on the k-most similar no. of instances performed on the trained model. It’s a non-parametric technique, as it doesn’t take any underlying assumption about data distribution. Hence, the accuracy of KNN algorithms is dependent on these three parameters.  \n",
        "\n",
        "• The no. of neighbors, ‘n_neighbors’  \n",
        "\n",
        "• Distance metric selection, ‘p’ = 1: Manhattan and 2: Euclidean \n",
        "• Weights, with each different contributing differently in terms of weight \n",
        "\n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig22.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 22\n",
        "\n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig23.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 23 \n",
        " \n",
        "\n",
        "## 5.2 Perform and Report Testing \n",
        "\n",
        "\n",
        "One of the goals of machine learning is to make great forecasts for new information on unknown information. Be that as it may, because of information limitation, assembling a model from the given informational collection is troublesome because of the inaccessibility of already inconspicuous information. In this way, the informational index is part of two distinct subsets:  \n",
        "\n",
        "Train - a subset to prepare a model.  \n",
        "\n",
        "Test - a subset to test the prepared model.  \n",
        "\n",
        "Efficiency in performance on the new data is commonly dictated by accurate execution on the test data. In view of the idea of the issue we have chosen to part the informational collection into 80%-20% train and test split separately utilizing 'sklearn.model_selection.train_test_split' work indicated by 'test_size' of 0.2 (20% test size), and utilizing stratification 'stratify = y'. Be that as it may, there is no thumb-principle of parting the dataset into 80%-20% train and test split. It's to a great extent subject to an individual/business decision or dependent on the designer to be actualized. \n",
        "\n",
        "#### KNN Test \n",
        "\n",
        "Clearly, the best K is the one that compares to the most reduced test error rate, so we should assume we complete rehashed estimations of the test blunder for various estimations of K. Incidentally, what we are doing is utilizing the test set as a preparation set. This implies we are belittling the genuine mistake rate since our model has been compelled to fit the test set in the most ideal way. Our model is then unequipped for summing up to more up to date perceptions, a procedure known as overfitting. Thus, contacting the test set is impossible and should just be done at the part of the arrangement.  \n",
        "\n",
        "Utilizing the test set for hyper parameter tuning can prompt overfitting.  \n",
        "\n",
        " An option and more brilliant methodology include assessing the test error rate by holding out a subset of the preparation set from the fitting procedure. This subset, called the approval set, can be utilized to choose the suitable degree of adaptability of our calculation. There are diverse approval approaches that are utilized practically speaking, and we will investigate one of the more well-known ones called k-fold cross approval.\n",
        " \n",
        " <img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig24.PNG?raw=true\" width=\"500\">\n",
        "\n",
        "Figure 24 \n",
        " \n",
        "\n",
        "## 5.3 Comparative Study \n",
        "\n",
        "In the below figure, comparisons in model is done with respect to accuracy of each model from 82% to 100%.  \n",
        "\n",
        "A subplot with key (args, kwargs) at that point it will just make that subplot current and return it. Then, on the off chance that you don't need this conduct (i.e., you need to constrain the making of another subplot), you should utilize an interesting arrangement of arguments and kwargs. The tomahawks name quality has been uncovered for this reason: on the off chance that you need two subplots that are generally indistinguishable from be added to the figure and ensure to assign distinct values. \n",
        "\n",
        "Plots on various types of models are presented, such as logistic regression, support vector machine, K nearest neighbour, naive bayes, decision tree and linear discriminant analysis.\n",
        "\n",
        "<img src=\"https://github.com/PrasadNagarajDesai/UTS_ML2019_ID13057765/blob/master/fig25.PNG?raw=true\" width=\"500\">\n",
        "\n",
        " Figure 25\n",
        "\n",
        "# Conclusion \n",
        "\n",
        "In the implementation of machine learning models, we used six different algorithms to generate models, the algorithms were linear and non-linear, and hence it was easy to evaluate the performance and understand the nature of various algorithms. The results were represented in form of graphs and plot diagrams using various visualisation techniques. After comparing the models, it was noticed that KNN model had highest accuracy. To recognise overfitting, 10 cross validation technique was used which helped in estimation of k value and assessing error rate. \n",
        "\n",
        "There are several ways to further improve the accuracy like using better pre-processing techniques, try to include more data attributes or using more relatable features. One can also tweak some parameters in the model function to get higher scores. \n",
        "\n",
        "# Ethical Consideration\n",
        "\n",
        "#### Utilitarian Approach\n",
        "\n",
        "As far as the dataset chosen, the topic is not directly corresponding to human understanding. The specialized issue here is that machine calculations are as of now naturally performing utilitarian sort counts. Calculations like KNN, Naive Bayes, Supports Vector Machines all base figure out how to take care of issues from model by limiting a blunder or hazard metric, or augmenting a benefit metric to make them moral, we would need to balance that with some standard based contemplations, not sloppy the waters by making them much increasingly utilitarian.  \n",
        "\n",
        " In a perfect world, I conquer, such machines ought to remain only advanced devices, furnishing individuals with target certainties while leaving a definitive basic leadership to people. Convoluted programming frameworks are as of now settling on choices all alone, for instance favouring advances and credit applications without human mediation. \n",
        "\n",
        " \n",
        "\n",
        "# References \n",
        "\n",
        "Cheng, W. and Hüllermeier, E., 2009. Combining instance-based learning and logistic regression for multilabel classification. Machine Learning, 76(2-3), pp.211-225.\n",
        "\n",
        " Kotsiantis, S.B., Zaharakis, I. and Pintelas, P., 2007. Supervised machine learning: A review of classification techniques. Emerging artificial intelligence applications in computer engineering, 160, pp.3-24.\n",
        "\n",
        "Zhang, M.L. and Zhou, Z.H., 2007. ML-KNN: A lazy learning approach to multi-label learning. Pattern recognition, 40(7), pp.2038-2048.\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Q-N0p8UnMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}